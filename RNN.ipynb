{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN tutorial\n",
    "\n",
    "This tutorial discusses how to develop RNN's in PyTorch, in particular RNNs for time series prediction. The following topics are covered:\n",
    "* Tensor structure for sequences\n",
    "* Vanilla RNN model in Pytorch\n",
    "* Stacked RNNs\n",
    "* Predicting n-day values\n",
    "\n",
    "The data used in the tutorial can be downloaded from: https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
    "\n",
    "__NOTE__: The code in this tutorial is sometimes unecessarily verbose to expose students to coding practices that are often helpful in developing machine learning pipelines. Where this is the case, the code will be marked with \"# verbose code\"\n",
    "\n",
    "Connect environment to a GPU by:\n",
    "* Select 'Runtime' in the top left\n",
    "* Select 'Change Runtime Type'\n",
    "* Select the GPU runtime available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from typing import Union, Callable, Tuple, List, Literal, Dict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "from abc import ABCMeta, abstractmethod\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tensor Structure for Sequences\n",
    "When working with sequences, such as time series or sentences, RNNs require input data to be formatted as tensors with specific dimensions. Understanding these tensor dimensions is crucial for developing RNN models.\n",
    "\n",
    "#### Tensor Dimensions in RNNs\n",
    "RNNs in PyTorch expect input tensors in a specific format. For a batch of sequences, the input tensor dimensions are:\n",
    "\n",
    "* Batch Size (`N`)**: The number of sequences in a batch.\n",
    "* Sequence Length (`T`)**: The length of each sequence (e.g., number of time steps in a time series).\n",
    "* Feature Size (`F`)**: The dimensionality of each input element (e.g., the number of features for each time step).\n",
    "\n",
    "Thus, the input tensor shape is `(N, T, F)` when `batch_first=True`. Otherwise, it is `(T, N, F)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch size of 2, sequence length of 5, feature size of 3\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "feature_size = 3\n",
    "\n",
    "# Create a random tensor with shape (batch_size, seq_length, feature_size)\n",
    "input_tensor = torch.randn(batch_size, seq_length, feature_size)\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)  # Output: (2, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow of Tensors Through RNN Models\n",
    "* Input to RNN Layer: The input tensor of shape (N, T, F) is fed into the RNN layer. The RNN processes each element in the sequence one time step at a time, using its internal hidden state, which is updated after processing each element.\n",
    "* Hidden State Initialization: At the beginning of the sequence processing, the hidden state is initialized (usually to zeros). The shape of the hidden state tensor is (num_layers * num_directions, N, hidden_size), where:\n",
    "  * num_layers is the number of RNN layers.\n",
    "  * num_directions is 2 if the RNN is bidirectional; otherwise, it is 1.\n",
    "  * hidden_size is the number of features in the hidden state.\n",
    "* Output of RNN Layer:\n",
    "  * Output Tensor: Shape (N, T, hidden_size) if batch_first=True. It contains the hidden states from the last RNN layer for each time step in the sequence.\n",
    "  * Hidden State Tensor: Shape (num_layers * num_directions, N, hidden_size), which contains the hidden state for the last time step of each layer. The number of hidden state is a hyperparameter.\n",
    "* Output to Fully Connected Layer:\n",
    "  * In many applications, we only care about the output at the final time step. Therefore, we often take the output tensor from the last time step (out[:, -1, :] if batch_first=True) and pass it to a fully connected layer to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([2, 5, 4])\n",
      "Hidden State Shape: torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# RNN configuration\n",
    "input_size = feature_size  # Number of input features per time step\n",
    "hidden_size = 4  # Number of features in the hidden state\n",
    "num_layers = 1  # Number of stacked RNN layers\n",
    "\n",
    "# Define the RNN layer\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "# Initialize hidden state\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "output, hn = rnn(input_tensor, h0)\n",
    "print(\"Output Tensor Shape:\", output.shape)  # Output: (2, 5, 4)\n",
    "print(\"Hidden State Shape:\", hn.shape)  # Output: (1, 2, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RNNs with Differing Sequence Length Inputs\n",
    "RNNs expect inputs to be in batches with a consistent sequence length for each batch. However, in many real-world scenarios, sequences have differing lengths. For instance:\n",
    "\n",
    "* In text processing, sentences can have different numbers of words.\n",
    "* In time-series data, different samples may have varying numbers of time steps.\n",
    "Handling these varying lengths in RNNs requires special techniques to ensure that shorter sequences are correctly processed without losing their shorter time-step information.\n",
    "\n",
    "##### Approaches to Handling Varying Sequence Lengths in RNNs\n",
    "* Padding Sequences: The most common approach to dealing with varying sequence lengths is to pad the shorter sequences so that all sequences in a batch have the same length. Padding is achieved by adding dummy values (typically zeros) to the end of shorter sequences. The padded sequences can then be processed by the RNN, but the padding itself should not affect the learning process. We can address this by using masking or packing techniques.\n",
    "* Packed sequences allow the RNN to efficiently ignore the padding by only processing the actual (non-padded) sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[10, 11, 12],\n",
      "         [13, 14, 15],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[16, 17, 18],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24],\n",
      "         [25, 26, 27]]])\n",
      "Packed output shape: torch.Size([3, 4, 4])\n",
      "Hidden state shape: torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Example input sequences with different lengths, each time step has 3 features\n",
    "sequences = [\n",
    "    torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),  # length 3 (3 time steps, each with 3 features)\n",
    "    torch.tensor([[10, 11, 12], [13, 14, 15]]),       # length 2\n",
    "    torch.tensor([[16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27]])  # length 4\n",
    "]\n",
    "\n",
    "# Step 1: Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "# Example sequence lengths\n",
    "lengths = torch.tensor([3, 2, 4])\n",
    "\n",
    "print(\"Padded sequences:\\n\", padded_sequences)\n",
    "\n",
    "# Step 2: Define RNN\n",
    "input_size = 3  # Number of features in each time step\n",
    "hidden_size = 4  # Number of features in the hidden state\n",
    "batch_size = len(sequences)\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "# Since the input_size is 1, we need to add an extra dimension to the input\n",
    "padded_sequences = padded_sequences.float()\n",
    "\n",
    "# Step 3: Pack the padded sequences\n",
    "packed_input = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "# Step 4: Pass through RNN\n",
    "h0 = torch.zeros(1, batch_size, hidden_size).float()  # Initialize hidden state as float\n",
    "packed_output, hn = rnn(packed_input, h0)\n",
    "\n",
    "# Step 5: Unpack the output\n",
    "output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "print(\"Packed output shape:\", output.shape)  # Shape: (batch_size, max_seq_len, hidden_size)\n",
    "print(\"Hidden state shape:\", hn.shape)        # Shape: (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above example code\n",
    "* Input Tensor: The input tensor is of shape (2, 5, 3), which matches the required (N, T, F) format.\n",
    "* RNN Output: The output tensor has the shape (2, 5, 4), which means for each of the 2 sequences (batch size), there are 5 time steps, and the hidden state has 4 features.\n",
    "* Hidden State (hn): The hidden state tensor at the last time step has the shape (1, 2, 4), representing 1 layer, 2 sequences, and a hidden state size of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/cs/academic/phd3/xinrzhen/xinran/TA'\n",
    "logger = logging.getLogger(\"rnn_tutorial\")\n",
    "logger.setLevel(logging.INFO)\n",
    "WANDB_PROJ = \"rnn_tutorial\"\n",
    "TMP_DIR = \"./tmp\"\n",
    "\n",
    "if os.path.isdir(TMP_DIR):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset\n",
    "The double for loop in the code block (also copied directly) below defines the train/test data structure (mentioned above):\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | \n",
    "\n",
    "* Generate datasets for different steps\n",
    "  * Setting a step causes columns with a step greater than 1 to be shorter than the original DataFrame. This is because the shift(-step) function shifts the data up a step row, which causes the last few rows of the data to become NaN when the step is greater than 1. \n",
    "  * These NaN values are usually discarded when dealing with prediction problems, reducing the amount of valid data.\n",
    "```python\n",
    "steps = [1,5,10]\n",
    "for df in [train_df, test_df]:\n",
    "    for stp in steps:\n",
    "        df[[f\"{col}_{stp}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-1*stp)\n",
    "```\n",
    "By resetting dataset following differents steps, \n",
    "\n",
    "However, it is generalised in the following ways:\n",
    "* To compute not just the meantemp at t+1 but all features at time t+1;\n",
    "* To compute t+step target values, not just t+1\n",
    "\n",
    "This generalisation is useful for performing __Exercise 6b__ and __Exercise 8__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   meantemp   humidity  wind_speed  meanpressure\n",
      "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
      "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
      "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
      "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
      "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTrain.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTest.csv\"))\n",
    "\n",
    "train_df[\"date\"] = pd.to_datetime(train_df[\"date\"], format=\"%Y-%m-%d\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
      "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
      "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
      "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
      "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
      "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
      "\n",
      "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
      "0        92.000000           2.980000          1017.800000         7.000000   \n",
      "1        87.000000           4.633333          1018.666667         7.000000   \n",
      "2        71.333333           1.233333          1017.166667         8.857143   \n",
      "3        86.833333           3.700000          1016.500000        14.000000   \n",
      "4        82.800000           1.480000          1018.000000        11.000000   \n",
      "\n",
      "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
      "0        82.800000           1.480000          1018.000000         15.714286   \n",
      "1        78.600000           6.300000          1020.000000         14.000000   \n",
      "2        63.714286           7.142857          1018.714286         15.833333   \n",
      "3        51.250000          12.500000          1017.000000         12.833333   \n",
      "4        62.000000           7.400000          1015.666667         14.714286   \n",
      "\n",
      "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
      "0         51.285714           10.571429           1016.142857  \n",
      "1         74.000000           13.228571           1015.571429  \n",
      "2         75.166667            4.633333           1013.333333  \n",
      "3         88.166667            0.616667           1015.166667  \n",
      "4         71.857143            0.528571           1015.857143  \n",
      "           date   meantemp    humidity  wind_speed  meanpressure  \\\n",
      "1457 2016-12-28  17.217391   68.043478    3.547826   1015.565217   \n",
      "1458 2016-12-29  15.238095   87.857143    6.000000   1016.904762   \n",
      "1459 2016-12-30  14.095238   89.666667    6.266667   1017.904762   \n",
      "1460 2016-12-31  15.052632   87.000000    7.325000   1016.100000   \n",
      "1461 2017-01-01  10.000000  100.000000    0.000000   1016.000000   \n",
      "\n",
      "      meantemp_1_step  humidity_1_step  wind_speed_1_step  \\\n",
      "1457        15.238095        87.857143           6.000000   \n",
      "1458        14.095238        89.666667           6.266667   \n",
      "1459        15.052632        87.000000           7.325000   \n",
      "1460        10.000000       100.000000           0.000000   \n",
      "1461              NaN              NaN                NaN   \n",
      "\n",
      "      meanpressure_1_step  meantemp_5_step  humidity_5_step  \\\n",
      "1457          1016.904762              NaN              NaN   \n",
      "1458          1017.904762              NaN              NaN   \n",
      "1459          1016.100000              NaN              NaN   \n",
      "1460          1016.000000              NaN              NaN   \n",
      "1461                  NaN              NaN              NaN   \n",
      "\n",
      "      wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
      "1457                NaN                  NaN               NaN   \n",
      "1458                NaN                  NaN               NaN   \n",
      "1459                NaN                  NaN               NaN   \n",
      "1460                NaN                  NaN               NaN   \n",
      "1461                NaN                  NaN               NaN   \n",
      "\n",
      "      humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
      "1457               NaN                 NaN                   NaN  \n",
      "1458               NaN                 NaN                   NaN  \n",
      "1459               NaN                 NaN                   NaN  \n",
      "1460               NaN                 NaN                   NaN  \n",
      "1461               NaN                 NaN                   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset with multiple step predictions\n",
    "non_date_vars = [col for col in train_df.columns if col != \"date\"]  # Exclude date column\n",
    "steps = [1, 5, 10]  # Predict future values at different steps\n",
    "for df in [train_df, test_df]:\n",
    "    for step in steps:\n",
    "        df[[f\"{col}_{step}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-step)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.tail())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split\n",
    "* The training dataset also needs to be split into a training and holdout set. When using any data where observations are non iid, data must be split to prevent \"data leakage\". Time series data is likely non-iid in the sense that future observations most likely depend on previous ones for example, it is reasonable to assume that the meantemp at time t+1 is dependant on the meantemp at time t.\n",
    "* The dataset therefore needs to be split such that the ML models is not explosed to correlations which would not be available at test time.\n",
    "* Given this, the final year of the training data is used as the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 17) (367, 17) (114, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
       "\n",
       "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
       "0        92.000000           2.980000          1017.800000         7.000000   \n",
       "1        87.000000           4.633333          1018.666667         7.000000   \n",
       "2        71.333333           1.233333          1017.166667         8.857143   \n",
       "3        86.833333           3.700000          1016.500000        14.000000   \n",
       "4        82.800000           1.480000          1018.000000        11.000000   \n",
       "\n",
       "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "0        82.800000           1.480000          1018.000000         15.714286   \n",
       "1        78.600000           6.300000          1020.000000         14.000000   \n",
       "2        63.714286           7.142857          1018.714286         15.833333   \n",
       "3        51.250000          12.500000          1017.000000         12.833333   \n",
       "4        62.000000           7.400000          1015.666667         14.714286   \n",
       "\n",
       "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "0         51.285714           10.571429           1016.142857  \n",
       "1         74.000000           13.228571           1015.571429  \n",
       "2         75.166667            4.633333           1013.333333  \n",
       "3         88.166667            0.616667           1015.166667  \n",
       "4         71.857143            0.528571           1015.857143  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and holdout sets to prevent data leakage\n",
    "train_df[\"__date_yrs\"] = train_df[\"date\"].dt.year\n",
    "val_idx = train_df[\"__date_yrs\"] >= 2016  # Holdout data for the last year\n",
    "val_df = train_df[val_idx].drop(columns=[col for col in train_df.columns if col.startswith(\"__\")])\n",
    "train_df = train_df[~val_idx].drop(columns=[col for col in train_df.columns if col.startswith(\"__\")])\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting next day values\n",
    "* For the first exercise, daily climate data will be used to develop a model that can predict the next day \"meantemp\".\n",
    "* First of all, training and test datasets need to be manipulated such that they are in the following form:\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the Vanilla RNN Model in PyTorch\n",
    "We will now implement a simple RNN model using PyTorch's nn.RNN module. This model will take sequences of historical data and predict future values.\n",
    "\n",
    "* Model Architecture\n",
    "  * RNN Layer: The core component of the model that processes sequences.\n",
    "  * Fully Connected Layer: A linear layer that maps the hidden states produced by the RNN to the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose code\n",
    "# Here a series of Debug classes are defined. The reason for using this structure will be discussed in class\n",
    "class DebugPass:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def debug(\n",
    "        self, \n",
    "        y_true:torch.tensor, \n",
    "        y_pred:torch.tensor, \n",
    "    ):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugBase(DebugPass):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def debug(self, y_true, y_pred):\n",
    "        self.y_true.append(y_true.cpu().detach().numpy())\n",
    "        self.y_pred.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugLocal(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        res_tbl.to_csv(os.path.join(TMP_DIR, \"validate_debug.csv\"), index=False)\n",
    "\n",
    "class DebugWandB(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        wandb_tbl = wandb.Table(dataframe=res_tbl)\n",
    "        wandb.log({\"val_predictions\" : wandb_tbl})\n",
    "\n",
    "def train_single_epoch(model:nn.Module, data_loader:torch.utils.data.DataLoader, \n",
    "                       gpu:Literal[True, False], optimizer:torch.optim,\n",
    "                       criterion:torch.nn.modules.loss\n",
    "                      ) -> Tuple[List[torch.Tensor]]:\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    preds = []\n",
    "    range_gen = tqdm(\n",
    "        enumerate(data_loader),\n",
    "        )\n",
    "    for i, (y,X) in range_gen:\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(X)\n",
    "        preds.append(output)\n",
    "        \n",
    "        train_loss = criterion(output, y)\n",
    "        losses.append(train_loss.item())\n",
    "\n",
    "        # losses.update(train_loss.data[0], g.size(0))\n",
    "        # error_ratio.update(evaluation(output, target).data[0], g.size(0))\n",
    "\n",
    "        try: \n",
    "            # compute gradient and do SGD step\n",
    "            train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(\"Runtime error on training instance: {}\".format(i))\n",
    "            raise e\n",
    "    return losses, preds\n",
    "\n",
    "def validate(model:nn.Module, data_loader:torch.utils.data.DataLoader,\n",
    "             gpu:Literal[True, False], criterion:torch.nn.modules.loss,\n",
    "             dh:DebugPass\n",
    "            ) -> Tuple[List[torch.Tensor]]:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        range_gen = tqdm(\n",
    "            enumerate(data_loader),\n",
    "        )\n",
    "        # Your code here\n",
    "        for i, (y,X) in range_gen:\n",
    "        \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(X)\n",
    "\n",
    "            # Logs\n",
    "            losses.append(criterion(output, y).item())\n",
    "            preds.append(output)\n",
    "            dh.debug(y_true=y, y_pred=output)\n",
    "    return losses, preds\n",
    "\n",
    "\n",
    "def train(model:torch.nn, train_data_loader:torch.utils.data.DataLoader,\n",
    "          val_data_loader:torch.utils.data.DataLoader, \n",
    "          gpu:Literal[True, False], optimizer:torch.optim,\n",
    "          criterion:torch.nn.modules.loss, epochs:int, \n",
    "          debug:bool = False, wandb_proj:str=\"\", \n",
    "          wandb_config:Dict={}\n",
    "         ) -> Tuple[List[torch.Tensor]]:\n",
    "\n",
    "    if (len(wandb_config) == 0) or (len(wandb_proj) == 0):\n",
    "        use_wandb = False\n",
    "        logger.warning(\"WandB not in use!\")\n",
    "        chkpnt_dir = TMP_DIR\n",
    "    else:\n",
    "        use_wandb = True\n",
    "        wandb.init(project=wandb_proj, config=wandb_config)\n",
    "        chkpnt_dir = wandb.run.dir\n",
    "\n",
    "    if debug:\n",
    "        if use_wandb:\n",
    "            dh = DebugWandB()\n",
    "        else:\n",
    "            dh = DebugLocal()\n",
    "    else:\n",
    "        dh = DebugPass()\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Running training epoch\")\n",
    "        train_loss_val, train_preds =  train_single_epoch(\n",
    "            model=model, data_loader=train_data_loader, gpu=gpu, \n",
    "            optimizer=optimizer, criterion=criterion)\n",
    "        mean_train_loss = np.mean(train_loss_val)\n",
    "        epoch_train_loss.append(mean_train_loss)\n",
    "        val_loss_val, val_preds = validate(\n",
    "            model=model, data_loader=val_data_loader, gpu=gpu, \n",
    "            criterion=criterion, dh=dh)\n",
    "        \n",
    "        print(\"Running validation\")\n",
    "        mean_val_loss = np.mean(val_loss_val)\n",
    "        epoch_val_loss.append(np.mean(val_loss_val))\n",
    "\n",
    "        chkp_pth = os.path.join(chkpnt_dir, f\"mdl_chkpnt_epoch_{epoch}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, chkp_pth)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train_loss\": mean_train_loss, \"val_loss\": mean_val_loss})\n",
    "            wandb.save(chkp_pth)\n",
    "    dh.close()\n",
    "    if use_wandb: \n",
    "        wandb.finish()\n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset class for time series data format\n",
    "* Using the PandasDataset class, data is batched according to the time dimension. This dataset object needs to be extended such that sequences of length longer than 1 can be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.Series, normalise:bool=True)->None:\n",
    "        self._X = torch.from_numpy(X.values).float()\n",
    "        if normalise:\n",
    "            self._X = self.__min_max_norm(self._X)\n",
    "        self.feature_dim = X.shape[1]\n",
    "        self._len = X.shape[0]\n",
    "        self._y = torch.from_numpy(y.values)[:,None].float()\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self._y[idx], self._X[idx,:]\n",
    "        \n",
    "    def __min_max_norm(self, in_tens:torch.Tensor) -> torch.Tensor:\n",
    "        # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        _min = in_tens.min(axis=0).values\n",
    "        _max = in_tens.max(axis=0).values\n",
    "        in_tens = (in_tens - _min)/(_max - _min)\n",
    "        return in_tens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 1 step mean temperature\n",
    "* For the first exercise, the aim is to predict the next day mean temperature using a historial time series of temperature, humidity, wind_speed and meanpressure values\n",
    "* A core hyperparameter is defining the sequence length i.e., the size of the historial time series to use for prediction.\n",
    "* As it stands, the data is of the form (time t obs, time t+1 target). Therefore, if this data was converted to a tensor as is, batched and used for training, only the time t values would be used for prediction. Using the \"PandasDataset\" class from the first tutorial demonstrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meantemp_1_step\n",
      "['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n"
     ]
    }
   ],
   "source": [
    "trgt_col = \"meantemp_1_step\" # This is the t+1 target associated with the time t observations\n",
    "# Dynamically select the remaining feature columns i.e., those that are: \n",
    "# 1) Not target variables (do not end with _step) and;\n",
    "# 2) Are not the date variable\n",
    "indp_cols = [ # verbose code\n",
    "    col for col in train_df.columns if (\n",
    "        (col != \"date\") and (col[-5:] != \"_step\")\n",
    "    )\n",
    "]\n",
    "print(trgt_col)\n",
    "print(indp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    meantemp   humidity  wind_speed  meanpressure  meantemp_1_step\n",
       "0  10.000000  84.500000    0.000000   1015.666667         7.400000\n",
       "1   7.400000  92.000000    2.980000   1017.800000         7.166667\n",
       "2   7.166667  87.000000    4.633333   1018.666667         8.666667\n",
       "3   8.666667  71.333333    1.233333   1017.166667         6.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first batch contains the first two rows of the dataset:\n",
      " tensor([[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "        [   7.4000,   92.0000,    2.9800, 1017.8000]])\n",
      "The second batch contains the second two rows of the dataset:\n",
      " tensor([[   7.1667,   87.0000,    4.6333, 1018.6667],\n",
      "        [   8.6667,   71.3333,    1.2333, 1017.1667]])\n"
     ]
    }
   ],
   "source": [
    "# Normalise has been set to False for demo purposes\n",
    "tmp_dataset = PandasDataset(X=train_df[indp_cols], y=train_df[trgt_col], normalise=False)\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False, batch_size=2)\n",
    "display(train_df[indp_cols+[trgt_col]].head(4))\n",
    "loader_iter = tmp_loader.__iter__()\n",
    "first_batch = next(loader_iter)\n",
    "print(f\"The first batch contains the first two rows of the dataset:\\n {first_batch[1]}\")\n",
    "second_batch = next(loader_iter)\n",
    "print(f\"The second batch contains the second two rows of the dataset:\\n {second_batch[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1a__: \n",
    "* The __get_lookback function is designed to augment the _X and _y tensors with sequences of length \"lookback\".\n",
    "* Where lookback is defined as 2, feature values at time points 't' and 't-1' are required to predict values at timepoint 't+1'.\n",
    "* The code contains a bug where the dimensions of the _X and _y are incorrect - fix this\n",
    "\n",
    "__Exercise 1b__: \n",
    "* Consider why the target tensor is indexed as follows ```i+lookback-1:i+lookback```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasTsDataset(PandasDataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, lookback: int, normalise: bool = True) -> None:\n",
    "        super().__init__(X=X, y=y, normalise=normalise)\n",
    "        if lookback > 1:\n",
    "            self.__get_lookback(lookback=lookback)\n",
    "        # Update the length to reflect the lookback window\n",
    "        self._len = self._X.shape[0]\n",
    "    \n",
    "    def __get_lookback(self, lookback: int):\n",
    "        X_vals = []\n",
    "        y_vals = []\n",
    "        # Ensure we stop correctly to avoid index out of bounds\n",
    "        for i in range(self._X.shape[0] - lookback + 1):\n",
    "            X_vals.append(self._X[i:i + lookback].unsqueeze(0))  # Add an extra dimension\n",
    "            y_vals.append(self._y[i + lookback - 1])  # Use the last value in the lookback period as the target\n",
    "        self._X = torch.cat(X_vals, axis=0)\n",
    "        self._y = torch.stack(y_vals, axis=0)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Return feature and target tensors for the given index\n",
    "        return self._y[idx], self._X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
       "\n",
       "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
       "0        92.000000           2.980000          1017.800000         7.000000   \n",
       "1        87.000000           4.633333          1018.666667         7.000000   \n",
       "2        71.333333           1.233333          1017.166667         8.857143   \n",
       "3        86.833333           3.700000          1016.500000        14.000000   \n",
       "4        82.800000           1.480000          1018.000000        11.000000   \n",
       "\n",
       "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "0        82.800000           1.480000          1018.000000         15.714286   \n",
       "1        78.600000           6.300000          1020.000000         14.000000   \n",
       "2        63.714286           7.142857          1018.714286         15.833333   \n",
       "3        51.250000          12.500000          1017.000000         12.833333   \n",
       "4        62.000000           7.400000          1015.666667         14.714286   \n",
       "\n",
       "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "0         51.285714           10.571429           1016.142857  \n",
       "1         74.000000           13.228571           1015.571429  \n",
       "2         75.166667            4.633333           1013.333333  \n",
       "3         88.166667            0.616667           1015.166667  \n",
       "4         71.857143            0.528571           1015.857143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the data contains the first and second row of the original dataset:\n",
      " tensor([[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "        [   7.4000,   92.0000,    2.9800, 1017.8000]])\n",
      "With the target defined as:\n",
      " tensor([7.1667])\n",
      "\n",
      "\n",
      "Second row of the data contains the second and third row of the original dataset:\n",
      " tensor([[   7.4000,   92.0000,    2.9800, 1017.8000],\n",
      "        [   7.1667,   87.0000,    4.6333, 1018.6667]])\n",
      "With the target defined as:\n",
      " tensor([8.6667])\n",
      "\n",
      "\n",
      "Final row of the data contains the penultimate and final row of the original dataset:\n",
      " tensor([[  15.5000,   71.7500,    2.1000, 1017.5000],\n",
      "        [  15.0000,   71.3750,    2.0875, 1020.5000]])\n",
      "With the target defined as:\n",
      " tensor([14.7143])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>15.375</td>\n",
       "      <td>63.250</td>\n",
       "      <td>7.8875</td>\n",
       "      <td>1020.625</td>\n",
       "      <td>17.125000</td>\n",
       "      <td>58.125000</td>\n",
       "      <td>10.887500</td>\n",
       "      <td>1020.875000</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>72.285714</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1021.142857</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>81.625000</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>17.125</td>\n",
       "      <td>58.125</td>\n",
       "      <td>10.8875</td>\n",
       "      <td>1020.875</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>7.412500</td>\n",
       "      <td>1018.125000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>75.875000</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>17.125000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1018.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>16.375</td>\n",
       "      <td>65.000</td>\n",
       "      <td>7.4125</td>\n",
       "      <td>1018.125</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>71.750000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1017.500000</td>\n",
       "      <td>14.375000</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>5.112500</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>7.887500</td>\n",
       "      <td>1017.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>15.500</td>\n",
       "      <td>71.750</td>\n",
       "      <td>2.1000</td>\n",
       "      <td>1017.500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>71.375000</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>77.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1017.625000</td>\n",
       "      <td>15.857143</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>8.471429</td>\n",
       "      <td>1015.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>15.000</td>\n",
       "      <td>71.375</td>\n",
       "      <td>2.0875</td>\n",
       "      <td>1020.500</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>72.285714</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1021.142857</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>88.833333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>74.375000</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1017.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  meantemp  humidity  wind_speed  meanpressure  \\\n",
       "1090 2015-12-27    15.375    63.250      7.8875      1020.625   \n",
       "1091 2015-12-28    17.125    58.125     10.8875      1020.875   \n",
       "1092 2015-12-29    16.375    65.000      7.4125      1018.125   \n",
       "1093 2015-12-30    15.500    71.750      2.1000      1017.500   \n",
       "1094 2015-12-31    15.000    71.375      2.0875      1020.500   \n",
       "\n",
       "      meantemp_1_step  humidity_1_step  wind_speed_1_step  \\\n",
       "1090        17.125000        58.125000          10.887500   \n",
       "1091        16.375000        65.000000           7.412500   \n",
       "1092        15.500000        71.750000           2.100000   \n",
       "1093        15.000000        71.375000           2.087500   \n",
       "1094        14.714286        72.285714           1.057143   \n",
       "\n",
       "      meanpressure_1_step  meantemp_5_step  humidity_5_step  \\\n",
       "1090          1020.875000        14.714286        72.285714   \n",
       "1091          1018.125000        14.000000        75.875000   \n",
       "1092          1017.500000        14.375000        74.750000   \n",
       "1093          1020.500000        15.750000        77.125000   \n",
       "1094          1021.142857        15.833333        88.833333   \n",
       "\n",
       "      wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "1090           1.057143          1021.142857         17.375000   \n",
       "1091           2.087500          1021.000000         17.125000   \n",
       "1092           5.112500          1018.500000         15.500000   \n",
       "1093           0.000000          1017.625000         15.857143   \n",
       "1094           0.616667          1017.000000         15.625000   \n",
       "\n",
       "      humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "1090         81.625000            2.312500           1016.500000  \n",
       "1091         87.000000            0.000000           1018.125000  \n",
       "1092         83.250000            7.887500           1017.250000  \n",
       "1093         65.142857            8.471429           1015.428571  \n",
       "1094         74.375000            2.775000           1017.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "The first batch contains input:\n",
      " tensor([[[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "         [   7.4000,   92.0000,    2.9800, 1017.8000]],\n",
      "\n",
      "        [[   7.4000,   92.0000,    2.9800, 1017.8000],\n",
      "         [   7.1667,   87.0000,    4.6333, 1018.6667]]])\n",
      "With target values:\n",
      " tensor([[7.1667],\n",
      "        [8.6667]])\n"
     ]
    }
   ],
   "source": [
    "tmp_dataset = PandasTsDataset(X=train_df[indp_cols], y=train_df[trgt_col], lookback=2, normalise=False)\n",
    "display(train_df.head())\n",
    "print(f\"First row of the data contains the first and second row of the original dataset:\\n {tmp_dataset[0][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[0][0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Second row of the data contains the second and third row of the original dataset:\\n {tmp_dataset[1][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[1][0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Final row of the data contains the penultimate and final row of the original dataset:\\n {tmp_dataset[-1][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[-1][0]}\")\n",
    "display(train_df.tail())\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False, batch_size=2)\n",
    "first_batch = next(tmp_loader.__iter__())\n",
    "print(f\"The first batch contains input:\\n {first_batch[1]}\")\n",
    "print(f\"With target values:\\n {first_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN model is now ready to be defined. To begin with, we'll try just using the nn.RNN module, provided by Pytorch. Consider the picture of an RNN below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks):\n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/generic_rnn_term_pred.png)\n",
    "\n",
    "The blue blocks represent a single RNN computation and each computation take a set of hidden values, $a^{<t-1>}$, an input $x^{<t>}$ and produces a hidden state itself $a^{<t>}$. The final computation in the sequence produces an output, $y$.\n",
    "\n",
    "* Mapping these to the input parameters of nn.RNN, \n",
    "    * input_size: Represents the dimenion of $x^{<t>}$ i.e., this represents the feature dimension for each observation in the input sequence\n",
    "    * hidden_size: Represents the dimension of the hidden layer within the RNN\n",
    "    * num_layers: Represents the number of \"stacked\" RNNs. Note this __does not__ represent the number of RNN computations. Ignore this parameter for now it is discussed later\n",
    "    * nonlinearity: Represents the non-linear function which produces the set of hidden values $a^{<t>}$\n",
    "    * batch_first: If set to True, tells the RNN to expect tensors of dimension (batch_size, sequence_size, feature_size) else it expects (sequence_size, batch_size, feature_size)\n",
    "    * bidirectional: If set to true a 'bidirectional' RNN is defined. This is out of scope for the tutorial\n",
    "\n",
    "The computation described by a single RNN unit is defined by:\n",
    "\\begin{equation}\n",
    "    a^{<t>} = \\textrm{nonlinearity}(x^{<t>}W_{i,h} + b_{i,h} + a^{<t-1>}W_{h,h} + b_{h,h})\n",
    "\\end{equation}\n",
    "\n",
    "__Exercise__ 2: The computation described in https://pytorch.org/docs/stable/generated/torch.nn.RNN.html and https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks are identical (however, the $b_{i,h}$ bias is set to the identity in the cheatsheet). Try to reconclie the two with pen and paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 3:\n",
    "* Using the description above, set the parameters below assuming we require:\n",
    "    * Sequences of length 2 for each input\n",
    "    * The dimension of $W_{h,h}$ to be 52\n",
    "    * relu activation functions\n",
    "    * Whether the input data should be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094\n",
      "1095\n",
      "Train Dataset: 1094 samples\n",
      "Validation Dataset: 366 samples\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "lookback = 2\n",
    "input_dim = len(indp_cols)\n",
    "hidden_dim = 52\n",
    "nonlinearity = \"relu\"\n",
    "shuffle=True\n",
    "num_layers = 1\n",
    "fc_output_size = 1  # Output size for regression task\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PandasTsDataset(X=train_df[indp_cols], y=train_df[trgt_col], lookback=lookback)\n",
    "val_dataset = PandasTsDataset(X=val_df[indp_cols], y=val_df[trgt_col], lookback=lookback)\n",
    "print(len(train_dataset))\n",
    "print(train_df[indp_cols].shape[0])\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=shuffle, batch_size=2)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=shuffle, batch_size=2)\n",
    "\n",
    "print(f\"Train Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Dataset: {len(val_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 4:\n",
    "* The code below produces a bug. Debug it and define the VanillaRNN class\n",
    "* _Hints_:\n",
    "    * Performing the reconciliation exercise will help with this, in particular noticing that the description in Pytorch (and therefore the computation implemented in the nn.RNN function is __missing__ the computation $y = g_{2}(W_{y,a}a^{<T>} + b_{y})$) where $a^{<T>}$ is the hidden layer output from the final RNN computation\n",
    "    * Also, examine the object type produced by the RNN() call. Is it what you expect? Have a look at the Pytorch documentation to understand what is being produced\n",
    "    * Finally, examine the output of the RNN model and the output of the dataloader - what do you obserse? (The code below will help do this)\n",
    " \n",
    "```python\n",
    "first_batch = next(train_loader.__iter__())\n",
    "print(f\"First batch shape: {first_batch[1].shape}\")\n",
    "print(f\"First batch obs:\\n{first_batch[1]}\")\n",
    "print(f\"First batch trgt:\\n{first_batch[0]}\")\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1])\n",
    "    print(f\"All hidden: {mdl_pred[0].shape}\")\n",
    "    print(f\"All hidden values:\\n {mdl_pred[0]}\")\n",
    "    print(f\"Final hidden: {mdl_pred[1].shape}\")\n",
    "    print(f\"Final hidden values:\\n {mdl_pred[1]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim:int,  hidden_dim:int, num_layers:int, \n",
    "                 fc_output_size:int, *args, **kwargs) -> None: \n",
    "        super().__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,  hidden_size=hidden_dim,\n",
    "            num_layers=num_layers, *args, **kwargs,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Your code here - this should represent y^{<t>} = g_{2}(W_{y,a}a^{<t>} + b_{y})\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=fc_output_size)\n",
    "        # Your code here - END\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out = self.rnn(x, hidden)\n",
    "        # Your code here - this should represent both equations from the stanford cheatsheet\n",
    "        return self.fc(out[1].squeeze())\n",
    "        # Your code here - END\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self._num_layers, batch_size, self._hidden_dim).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WandB not in use!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 363.21it/s]\n",
      "183it [00:00, 1993.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 348.30it/s]\n",
      "183it [00:00, 1333.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 335.70it/s]\n",
      "183it [00:00, 1593.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 307.72it/s]\n",
      "183it [00:00, 1294.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 300.90it/s]\n",
      "183it [00:00, 1378.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, fc_output_size=fc_output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "lr = 0.0001\n",
    "gpu = True\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion=nn.MSELoss()\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch shape: torch.Size([2, 2, 4])\n",
      "First batch obs:\n",
      "tensor([[[0.5731, 0.5034, 0.1042, 0.6601],\n",
      "         [0.5502, 0.4516, 0.1152, 0.6482]],\n",
      "\n",
      "        [[0.7795, 0.2595, 0.3292, 0.5059],\n",
      "         [0.8062, 0.1900, 0.2742, 0.4822]]])\n",
      "First batch trgt:\n",
      "tensor([[24.8750],\n",
      "        [31.1250]])\n",
      "All hidden: torch.Size([1])\n",
      "All hidden values:\n",
      " tensor([16.8468], device='cuda:0')\n",
      "Final hidden: torch.Size([1])\n",
      "Final hidden values:\n",
      " tensor([16.8486], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(train_loader.__iter__())\n",
    "print(f\"First batch shape: {first_batch[1].shape}\")\n",
    "print(f\"First batch obs:\\n{first_batch[1]}\")\n",
    "print(f\"First batch trgt:\\n{first_batch[0]}\")\n",
    "\n",
    "\n",
    "first_batch_inputs = first_batch[1].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch_inputs) \n",
    "    print(f\"All hidden: {mdl_pred[0].shape}\")\n",
    "    print(f\"All hidden values:\\n {mdl_pred[0]}\")\n",
    "    print(f\"Final hidden: {mdl_pred[1].shape}\")\n",
    "    print(f\"Final hidden values:\\n {mdl_pred[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred y: torch.Size([2, 1])\n",
      "True y: torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_024024-gpr04c2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/gpr04c2e' target=\"_blank\">lunar-lion-29</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/gpr04c2e' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/gpr04c2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 461.17it/s]\n",
      "183it [00:00, 1313.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 495.44it/s]\n",
      "183it [00:00, 1382.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 371.70it/s]\n",
      "183it [00:00, 1879.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 321.13it/s]\n",
      "183it [00:00, 1252.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 330.51it/s]\n",
      "183it [00:00, 1362.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>45.19551</td></tr><tr><td>val_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-lion-29</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/gpr04c2e' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/gpr04c2e</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 6 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_024024-gpr04c2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_config={\n",
    "    \"lr\": 0.0001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback,\n",
    "    \"nonlinearity\":nonlinearity\n",
    "}\n",
    "model = VanillaRNN(\n",
    "    input_dim=input_dim,  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"], \n",
    "    nonlinearity=nonlinearity\n",
    ")\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1].to(device))\n",
    "    print(f\"Pred y: {mdl_pred.shape}\")\n",
    "    print(f\"True y: {first_batch[0].shape}\")\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion=nn.MSELoss()\n",
    "epochs = 5\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 5:\n",
    "* Notice under \"Run summary\" a \"nan\" is returned. The training loop provided at the beginning of this script has been augmented with the functionality to push the ground truth values and predicted values from the validation set to weights and biases. Use weights and biases to debug why nans are being produced in the validation and implement the fix.\n",
    "* _Hint_:\n",
    "    * Examine the validation ground truth closely - try exporting it to a csv!\n",
    "\n",
    "The code explicitly filters out data points that are NaN in the target column (trgt_col), thus ensuring that the data input to the model is complete and has no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na_idx = ~train_df[trgt_col].isna()\n",
    "train_dataset = PandasTsDataset(\n",
    "    X=train_df[non_na_idx][indp_cols], \n",
    "    y=train_df[non_na_idx][trgt_col],\n",
    "    lookback=lookback\n",
    ")\n",
    "non_na_idx = ~val_df[trgt_col].isna()\n",
    "val_dataset = PandasTsDataset(\n",
    "    X=val_df[non_na_idx][indp_cols], \n",
    "    y=val_df[non_na_idx][trgt_col],\n",
    "    lookback=lookback\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=shuffle, batch_size=2)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=shuffle, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_030633-vh22vb7i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/vh22vb7i' target=\"_blank\">iconic-snowball-31</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/vh22vb7i' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/vh22vb7i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 501.62it/s]\n",
      "141it [00:00, 1405.49it/s]/scratch1/NOT_BACKED_UP/xinrzhen/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "183it [00:00, 1361.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 308.13it/s]\n",
      "183it [00:00, 1404.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 325.76it/s]\n",
      "183it [00:00, 1461.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 293.44it/s]\n",
      "183it [00:00, 1410.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:01, 322.46it/s]\n",
      "183it [00:00, 1477.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>44.07481</td></tr><tr><td>val_loss</td><td>29.28804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-snowball-31</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/vh22vb7i' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/vh22vb7i</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_030633-vh22vb7i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_config={\n",
    "    \"lr\": 0.0001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback, \n",
    "    \"nonlinearity\":nonlinearity\n",
    "}\n",
    "model = VanillaRNN(\n",
    "    input_dim=len(indp_cols),  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "    nonlinearity=nonlinearity\n",
    ")\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion=nn.MSELoss()\n",
    "epochs = 5\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the size of the lookback\n",
    "A working RNN model for one step predict has been defined. \n",
    "\n",
    "__Exercise__ 6a:\n",
    "* Try improving performance of the model by changing the lookback size: Here, we are trying to define the 'amount' of historial time steps relevant for prediction\n",
    "\n",
    "__Exercise__ 6b:\n",
    "* Try experimenting with the output of the model. In machine learning \"auxiliary loss functions\" are often used to improve performance. Auxiliary loss functions assess the performance of a model to do a related task in order to increase the amount of gradient signal to pass to the model. For example, in healthcare, when developing a model to predict acute kidney injury (AKI), DeepMind assessed whether the model could predict the outcome of the lab test for AKI (https://www.nature.com/articles/s41586-019-1390-1). It might be reasonable to assume that predicting the next day values for humidity, windspeed and pressure would help in the prediction for mean temperature.\n",
    "* An alternate auxiliary might be to keep meantemp as the prediction target but predict the intermediatary days as well i.e., defining an RNN of the form:\n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/generic_rnn.png)\n",
    "\n",
    "* _Hint_:\n",
    "    * The first auxiliary loss will require significant modifications to pretty much all of the steps above - don't worry if you're rewriting a lot of code!\n",
    "    * The second auxiliary loss only requires alterating the indexing in the PandasTsDataset function and input dimensions to the fully connected head. Alternatively, would it be better to use a specific head for each intermeditey output?\n",
    "    * When validating, we are still only interested in the ability for the model to predict the temperature!\n",
    "    * The hyperparameters previously discussed i.e., learning rate, epochs, batch_size and the network architecture might need to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_031012-om5tlyvu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/om5tlyvu' target=\"_blank\">devoted-dragon-32</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/om5tlyvu' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/om5tlyvu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [00:01, 327.92it/s]/scratch1/NOT_BACKED_UP/xinrzhen/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "546it [00:01, 320.72it/s]\n",
      "181it [00:00, 1695.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:01, 338.41it/s]\n",
      "181it [00:00, 1313.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:01, 307.79it/s]\n",
      "181it [00:00, 1413.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:01, 357.80it/s]\n",
      "181it [00:00, 1535.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:01, 505.59it/s]\n",
      "181it [00:00, 1418.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>6.71398</td></tr><tr><td>val_loss</td><td>16.41128</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-dragon-32</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/om5tlyvu' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/om5tlyvu</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_031012-om5tlyvu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_031030-6b009fx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/6b009fx5' target=\"_blank\">fine-resonance-33</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/6b009fx5' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/6b009fx5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 502.45it/s]\n",
      "0it [00:00, ?it/s]/scratch1/NOT_BACKED_UP/xinrzhen/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "179it [00:00, 2335.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 543.76it/s]\n",
      "179it [00:00, 2236.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 509.05it/s]\n",
      "179it [00:00, 2129.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 516.69it/s]\n",
      "179it [00:00, 1749.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 574.77it/s]\n",
      "179it [00:00, 1864.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>7.40396</td></tr><tr><td>val_loss</td><td>12.06799</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-resonance-33</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/6b009fx5' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/6b009fx5</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_031030-6b009fx5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lookback in [5,10]:\n",
    "    non_na_idx = ~train_df[trgt_col].isna()\n",
    "    train_dataset = PandasTsDataset(\n",
    "        X=train_df[non_na_idx][indp_cols], \n",
    "        y=train_df[non_na_idx][trgt_col],\n",
    "        lookback=lookback\n",
    "    )\n",
    "    non_na_idx = ~val_df[trgt_col].isna()\n",
    "    val_dataset = PandasTsDataset(\n",
    "        X=val_df[non_na_idx][indp_cols], \n",
    "        y=val_df[non_na_idx][trgt_col],\n",
    "        lookback=lookback\n",
    "    )\n",
    "    train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=2)\n",
    "    val_loader = DataLoader(dataset=val_dataset, shuffle=True, batch_size=2)\n",
    "    \n",
    "    wandb_config={\n",
    "        \"lr\": 0.0001,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"num_layers\": 1,\n",
    "        \"fc_output_size\": 1,\n",
    "        \"lookback\": lookback,\n",
    "        \"nonlinearity\":nonlinearity\n",
    "    }\n",
    "    model = VanillaRNN(\n",
    "        input_dim=len(indp_cols),  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "        num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "        nonlinearity=nonlinearity\n",
    "    )\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "    criterion=nn.MSELoss()\n",
    "    epochs = 5\n",
    "    epoch_train_loss, epoch_val_loss = train(\n",
    "        model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "        optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "        wandb_config=wandb_config, debug=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNNs\n",
    "The nn.RNN module also contains a 'num_layers' parameter. Setting 'num_layers' to greater than 1 creates a \"stacked\" RNN which is depicted below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) \n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/rnn_stacked.png)\n",
    "\n",
    "Stacking RNNs is similar to making MLPs deeper. One may want to stack an RNN if the raw features have different 'levels' of time dependant features as each layer extracts a non-linear relationship between the input to that layer and each layer is also recursively defined for the input sequence. For the climate example here, a stacked RNN might be required if it was hypothesised that there existed 'more complex' interactions between the four input variables than can be captured by a single non-linear layer. Furthermore, these 'more complex' interactions would have to be themselves recursive else, a deeper MLP could just be used instead to extract the time t prediction, $y_{i}$.\n",
    "\n",
    "__Exercise__ 7:\n",
    "* Experiment with different numbers of RNN layers and MLP head layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, fc_output_size: int, *args, **kwargs) -> None:\n",
    "        super(StackedRNN, self).__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define a stacked RNN with multiple layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=fc_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Forward propagate through the stacked RNN\n",
    "        out, hidden = self.rnn(x, hidden)  # out: (batch_size, seq_len, hidden_dim), hidden: (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # Take the output from the last time step for prediction\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output for all batches\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        hidden = torch.zeros(self._num_layers, batch_size, self._hidden_dim).to(device)\n",
    "        return hidden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_031626-glm7f239</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/glm7f239' target=\"_blank\">graceful-river-34</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/glm7f239' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/glm7f239</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 313.01it/s]\n",
      "179it [00:00, 1128.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 357.36it/s]\n",
      "179it [00:00, 907.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 330.76it/s]\n",
      "179it [00:00, 1011.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 320.13it/s]\n",
      "179it [00:00, 929.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 277.54it/s]\n",
      "179it [00:00, 887.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>7.50963</td></tr><tr><td>val_loss</td><td>12.89885</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-river-34</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/glm7f239' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/glm7f239</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_031626-glm7f239/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_config={\n",
    "    \"lr\": 0.0001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback, \n",
    "    \"nonlinearity\":nonlinearity\n",
    "}\n",
    "model = StackedRNN(\n",
    "    input_dim=len(indp_cols),  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "    nonlinearity=nonlinearity\n",
    ")\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion=nn.MSELoss()\n",
    "epochs = 5\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting n step values\n",
    "\n",
    "__Exercise__ 8:\n",
    "* Experiment with using the other target columns i.e. meantemp_5_step. When using meantemp_5_step as the target variable, we are building a model can can predict the temperature 5 days in advance. Using auxiliary losses might be useful here as one may expect that if the model can predict the next day more accurately, it should be able to predict the fifth day more accurately. However, again be careful not to use the 1 day predictions in your validation assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNMulti(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, fc_output_size, output_steps, nonlinearity='tanh'):\n",
    "        super(VanillaRNNMulti, self).__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self.output_steps = output_steps  # Store output_steps as a class attribute\n",
    "\n",
    "        # Initialize nn.RNN correctly without passing output_steps\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,  \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            nonlinearity=nonlinearity,\n",
    "            batch_first=True  # Ensure that the batch dimension is the first dimension\n",
    "        )\n",
    "\n",
    "        # Define a fully connected layer for the output\n",
    "        self.fc = nn.Linear(hidden_dim, fc_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(self._num_layers, batch_size, self._hidden_dim).to(x.device)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Select the output from the last time step or customize for multiple time steps\n",
    "        # Here we only take the output of the last time step as an example\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 1086 samples\n",
      "Validation Dataset: 358 samples\n",
      "Test Dataset: 105 samples\n"
     ]
    }
   ],
   "source": [
    "lookback = 10  # Example lookback period, the amount of data used in the past\n",
    "target_columns = [\"meantemp_1_step\", \"meantemp_5_step\"]  # Predict multiple future steps\n",
    "input_columns = [col for col in train_df.columns if col not in target_columns + [\"date\"]]  # Exclude target columns and date\n",
    "\n",
    "# Create datasets with multiple target columns for auxiliary loss\n",
    "train_dataset = PandasTsDataset(X=train_df[input_columns], y=train_df[target_columns], lookback=lookback)\n",
    "val_dataset = PandasTsDataset(X=val_df[input_columns], y=val_df[target_columns], lookback=lookback)\n",
    "test_dataset = PandasTsDataset(X=test_df[input_columns], y=test_df[target_columns], lookback=lookback)\n",
    "\n",
    "print(f\"Train Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred y shapes: [torch.Size([1]), torch.Size([1])]\n",
      "True y: torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/academic/phd3/xinrzhen/xinran/TA/wandb/run-20241025_031821-u7ipblqf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/u7ipblqf' target=\"_blank\">hardy-field-35</a></strong> to <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/u7ipblqf' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/u7ipblqf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 499.59it/s]\n",
      "179it [00:00, 1999.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 697.98it/s]\n",
      "179it [00:00, 2032.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 640.26it/s]\n",
      "179it [00:00, 1945.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 439.37it/s]\n",
      "179it [00:00, 1894.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:01, 488.48it/s]\n",
      "179it [00:00, 2026.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>7.24605</td></tr><tr><td>val_loss</td><td>12.02279</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-field-35</strong> at: <a href='https://wandb.ai/malware_detection/rnn_tutorial/runs/u7ipblqf' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial/runs/u7ipblqf</a><br/> View project at: <a href='https://wandb.ai/malware_detection/rnn_tutorial' target=\"_blank\">https://wandb.ai/malware_detection/rnn_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 6 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_031821-u7ipblqf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for n-step prediction\n",
    "output_steps = 5  # Number of steps to predict (e.g., 1-step and 5-step)\n",
    "wandb_config = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback,\n",
    "    \"nonlinearity\": nonlinearity,\n",
    "    \"output_steps\": output_steps\n",
    "}\n",
    "\n",
    "model = VanillaRNNMulti(\n",
    "    input_dim=input_dim,  \n",
    "    hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], \n",
    "    fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "    output_steps=wandb_config[\"output_steps\"],  # Specify the number of output steps\n",
    "    nonlinearity=nonlinearity\n",
    ").to(device)\n",
    "\n",
    "# Test the model's forward pass with dummy data\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1].to(device))\n",
    "    print(f\"Pred y shapes: {[p.shape for p in mdl_pred]}\")\n",
    "    print(f\"True y: {first_batch[0].shape}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 5\n",
    "\n",
    "# Train and validate the model\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu=gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
